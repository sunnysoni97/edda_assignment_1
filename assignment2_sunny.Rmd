---
title: "assignment2_sunny"
author: "Sunny"
date: '2022-03-10'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question4

Consult lecture 7 for contingency part

```{r}
psi_data = read.table("psi.txt",header=TRUE)
#psi_data$psi = as.factor(psi_data$psi)
#psi_data$passed = as.factor(psi_data$passed)
summary(psi_data)
```

#### Part 1

```{r}
model_g <- glm(passed~psi+gpa,data=psi_data, family=binomial)
summary(model_g)
```

```{r}
model_l <- lm(passed~psi+gpa,data=psi_data)
anova(model_l)

```

Using the summary of linear model as well as anova tests, we can clearly see that both factors psi and gpa, have a significant effect on the response variable "passed".

From our tests, we can say that psi works.

#### Part 2

```{r}
increased_odds_psi = -11.6+2.34+(3.06*3.0)
prob_with_psi = 1/(1+exp(abs(increased_odds_psi)))
print(prob_with_psi)

increased_odds_non_psi = -11.6+(3.06*3.0)
prob_without_psi = 1/(1+exp(abs(increased_odds_non_psi)))
print(prob_without_psi)
```

Using the summary of linear model, we can calculate the probability that a student who receives psi and has a GPA equal to 3.0 has a probability of 0.48, while the student not having it has a probability of 0.08.

#### Part 3

```{r}
#ODDS WITH PSI

odds_with_psi = -11.60 + 2.34
p_odds_with_psi = 1/(1+exp(abs(odds_with_psi)))
print(p_odds_with_psi)


#ODDS WITHOUT PSI
odds_without_psi = -11.60
p_odds_wout_psi = 1/(1+exp(abs(odds_without_psi)))
print(p_odds_wout_psi)

#RELATIVE CHANGE
rel_change = (p_odds_with_psi-p_odds_wout_psi)/p_odds_wout_psi
print(rel_change)
```

Hence, the relative odds increase 9.38 times for passing if the student has psi.

No, the relative change calculated is not dependent on GPA because we have not included that factor for the estimation of the odds.

#### Part 4

```{r}
non_passed = psi_data[which(psi_data$passed == "0"),]
passed = psi_data[which(psi_data$passed == "1"),]

#filtering data
psi_passed = passed[which(passed$psi == "1"),]
non_psi_passed = passed[which(passed$psi == "0"),]
psi_non_passed = non_passed[which(non_passed$psi == "1"),]
non_psi_non_passed = non_passed[which(non_passed$psi == "0"),]

#putting in matrix

con_mat = matrix(data=c(lengths(psi_passed)[1],lengths(psi_non_passed)[1],lengths(non_psi_passed)[1],lengths(non_psi_non_passed)[1]),byrow = TRUE,nrow=2,ncol=2,dimnames = list(c("PSI","NON-PSI"),c("PASSED","NON-PASSED")))

print(con_mat)

```

```{r}
fisher.test(con_mat)
```

Since the p-value\<0.03 in the fisher's exact test, we can reject the null hypothesis and accept the alternate hypthesis that there is dependence of type of student (psi/non-psi) on the outcome if he passes or does not pass. There is significant dependence between row and column factors.

Odds with student having psi and passed = 8/32

Odds with student having non-psi and passed = 3/32

```{r}
p_1 = 8/32
p_2 = 3/32
rel_change_2 = (p_1-p_2)/p_2
print(rel_change_2)
```

As we can see, the relative change in odds is 1.67 times, meaning the person has 1.67 times the chance of passing the test if he/she takes psi.

The answer is quite different compared to first approach where it was 9.38 times increase.

#### Part 5

No, the second approach is not wrong (contingency table and fisher's test) and is acceptable because of small number of data points and more accurate in this case.

**Advantages of second approach (contingency tables):**

Since data is aggregated into matrix, its very easy to calculate probabilities if tests like fisher test and chisquare test indicate significant dependence of row variables onto column ones.

**Disadvantage:**

Simpler form of calculations can be misleading for data with a large number of factors, harder to represent in matrix form. Doesnt work with large data.

**Advantages of first approach (Linear Regression) :**

Complexity and precision with ability to take in large number of factors. Large data handling possible.

**Disadvantages:**

Probability calculations are complex and computationally harder. Not very accurate with less number of data points.
