---
title: "Experimental Design and Data Analysis - Assignment 1"
author: "Group 75 - Sunny Soni, Maxine Palmen, Muska Neek"
date: '2022-02-28'
output: 
  pdf_document: 
    keep_tex: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1

#### **1A**

Below, the measured waiting time in the waiting room of doctor's office is given of 15 people. This waiting time is represented in a vector of length 15, where the mean waiting time is a little more than 11 minutes.

```{r waiting_t}
waiting_t <- c(15.4, 17.9, 19.0, 0.5, 15.9, 2.7, 6.2, 2.5, 4.7, 6.9, 10.8, 24.3, 5.6, 23.0, 10.7)
mu_t = mean(waiting_t)
format(round(mu_t, 3), nsmall = 3) #rounding off to 3 decimals places 
```

There are several ways to check normality of the waiting time data, such as a histogram, QQ-plot, Shapiro-wilk Test.The Histogram and QQ-plot are graphical ways to check normality, and the Shapiro-wilk test is a statistical test to check normality. In the Shapiro-wilk test, the null-hypothesis claims that the sample data comes from a normal distribution, and the alternative hypothesis claims that the sample data does not come from normal distribution. This means that when the p-value is above the confidence level of $\alpha=0.05$, that the sample data does come from normal distribution since the null hypothesis cannot be rejected.

Below, we will perform all of these normality checks.

```{r, echo=FALSE}
par(mfrow=c(1,2))
hist(waiting_t, main='Histogram of Waiting time')
qqnorm(waiting_t, main='QQ-Plot of Waiting Time')
qqline(waiting_t)
```

According to the histogram, it seems that the waiting time data does not come from a normal distribution. However, the sample size is very small, which could influence this graphical test. The QQ-plot shows a quite straight line of the sample data and therefore this could mean that the sample data comes from normal distribution. To make the normality check final, the Shapiro-Wilk test is performed to accept this assumption that the sample data comes from normal distribution.

```{r, echo=FALSE, results='hide'}
options(digits=5)
shapiro.test(waiting_t)
```

The result of the Shapiro-wilk test shows a p-value of 0.3. This means that the data of the waiting time is normally distributed. To construct a 97%-CI, the following formula needs to be computed: mean(sample) +- z\*se where se is the standard error which is computed by: sd/sqrt(n) where n is the sample size.

```{r, echo=FALSE, results='hide'}
#97% CI means an alpha of 0.03
z <- qnorm(0.985) #get z(alpha/2)
me_t <- z*((sd(waiting_t))/sqrt(15))
Lb <- round((mu_t - me_t), 3)
Ub <- round((mu_t + me_t), 3)
sprintf("%f is Lower Bound", Lb)
sprintf("%f is Upper Bound",Ub)
```

The upper and lower bound of 97%-CI is [6.743, 15.403]

```{r, echo=FALSE, results='hide'}
#97% CI means an alpha of 0.03
z <- qnorm(0.985) #get z(alpha/2)
sd_t <- (sd(waiting_t))
E = 2
sample_size <- round((z^2 * sd_t^2/E^2), 3)
sprintf("%f is minimum sample size",sample_size)
```

Bootstrap

```{r, results='hide'}
B=1000
Tstar = numeric(B)
for(i in 1:B){
  Xstar=sample(waiting_t, replace=TRUE)
  Tstar[i]=mean(Xstar)}
Tstar15=quantile(Tstar, 0.015)
Tstar985=quantile(Tstar, 0.985)
sum(Tstar<Tstar15)
round(c(2*mu_t-Tstar985, 2*mu_t-Tstar15), 3)
```

The 97% bootstrap CI is [6.61, 14.86] The original CI was [6.743, 15.403], which means that the bootstrap 97%-CI and the original 97%-CI do not differ much.

#### **1B**

Claim: mean waiting time \< 15 H0: mean waiting time \>= 15 H1: mean waiting time \< 15 Since the data comes from a normal distribution (see normality check at top of document), a t-test can be used. We only have one data sample, and therefor perform a one sample t-test.

```{r, echo=FALSE, results='hide'}
t.test(waiting_t, mu =15, alternative="less")
```

The p-value for this one-sample t-test is 0.03, which means that the null-hypothesis can be rejected and that the mean of the data sample indeed is less than 15. The CI-interval for this t-test is [-$\infty$, 14.6], which means that we can be 95% sure that the mean of the data sample lies within the CI-interval.

In the sign test we care about the median instead of the mean. Our hypotheses H0 and H1 are still the same as in the one-sample t-test, but instead of mean we use median. The hypothesized median is 15.

```{r, echo=FALSE, results='hide'}
above_m <- sum(waiting_t<15)
binom.test(above_m, length(waiting_t), p=0.5, alternative = "less")
```

For this problem, the Wilcoxon signed rank test could also be performed. This test needs a sample from a symmetric population and cares about the median.

#### **1C**

Compute powers of t-test and sign test

```{r, echo=FALSE, results='hide'}
sd = sd(waiting_t)
B=1000; n= 50
psign= numeric(B)
pttest = numeric(B)
for(i in 1:B) {
  x=rnorm(n,mean=13, sd = sd) ## generate data under H1 with mu=13
  pttest[i]=t.test(x, mu =15, alternative="less")[[3]] ## extract p-value
  above_m <- sum(x<15)
  psign[i]=binom.test(above_m, n, p=0.5, alternative = "less")[[3]]}
sum(psign<0.05)/B
sum(pttest<0.05)/B
B=1000; n= 50
psign2= numeric(B)
pttest2 = numeric(B)
for(i in 1:B) {
  x=rnorm(n,mean=14,sd= sd) ## generate data under H1 with mu=14
  above_m <- sum(x<15)
  pttest2[i]=t.test(x, mu =15, alternative="less")[[3]] ## extract p-value
  psign2[i]=binom.test(above_m, n, p=0.5, alternative = "less")[[3]]
  
  }
sum(psign2<0.05)/B
sum(pttest2<0.05)/B
```

By comparing the power of the t-test and the sign-test, we can see that the power of the t-test is higher than the power of the sign test. Therefore, the t-test is a better test for this application. A reason for this is that a t-test depends on normality and the data comes from a normal distribution. So, therefore the t-test is more reliable.

#### **1D**

```{r, echo=FALSE, results='hide'}
n = length(waiting_t) #sample size
p_hat_r = 0.53 #right end of CI
point_est = (sum(waiting_t>15.5))/n #probability that people have to wait longer than 15.5 minutes
SE= sqrt(point_est*(1-point_est)/n) #Calculating standard error of proportions

#point estimate + m = 0.53
#m = 0.53 - point estimate
m = p_hat_r - point_est
p_hat_l = point_est - m 
CI_p = c(p_hat_l, p_hat_r)
print(CI_p)
#calculate confidence level
#m = z * SE
#confidence level can be calculated using z
#z = m/SE
z = m/SE
CL = pnorm(z)-(1-pnorm(z)) #calculating 2 sided confidence level
CL
```

The recovered confidence interval of the probability that a patient has to wait longer than 15.5 minutes is [0.137, 0.53], and this has a confidence level of 89.39%

#### **1E**

Test of proportions H0: Proportion of long waiters of man is equal to proportion of long waiters of woman Ha: Proportions are different

```{r, echo=FALSE, results='hide'}
prop.test(x=c(3, 2), n = c(7, 8))
```

The p-value is 0.853, which is much larger than 0.05. Therefore, the waiting time for man and woman are the same.

## Question 2

#### **2A**

The Kolmogorov-Smirnov normality test examines if variables are normally distributed, which it is not because the data is exponentially distributed. The Mann-Whitney U test is used to compare whether there is a difference in the dependent variable for two independent groups. It compares whether the distribution of the dependent variable is the same for the two groups and therefore from the same population. The setting needs to be symmetric, which our data is clearly not, therefore not applicable. A t-test can only be used when comparing the means of two groups (a.k.a. pairwise comparison), which is applicable.

```{r}
#check for normality, which they are not as seen in the following tests
clouds=read.table(file="clouds.txt",header=TRUE)
seeded = clouds[,1]
unseeded = clouds [,2]
par(mfrow=c(1,3))
hist(seeded)
boxplot(seeded)
qqnorm(seeded)
qqline(seeded)
shapiro.test(seeded) # extra test

hist(unseeded)
boxplot(unseeded)
qqnorm(unseeded)
qqline(unseeded)
shapiro.test(unseeded) #extra test
t.test(seeded, unseeded)
wilcox.test(seeded,unseeded)
ks.test(seeded,unseeded)
```

#### **2B**

Repeat the procedures from a) first on the square root of the values in clouds.txt, then on the square root of the square root of the values in clouds.txt. Comment on your findings.

Except for the t-test whereas the mean of both values are smaller, the p-values are unchanged in the Mann-whitney and Kolmogorov-Smirnov test, which is plausible and explainable when looking at their formulas, because it takes the proportions into consideration. Therefore leading to the same pvalue.

```{r}
t.test(sqrt(seeded), sqrt(unseeded))
wilcox.test(sqrt(seeded),sqrt(unseeded))
ks.test(sqrt(seeded),sqrt(unseeded))
```

#### **2C**

$H_0$ : data is from a exponential distribution given that p\> 0.05 , also in KS-test, then the $H_0$can not be rejected. Therefore, in conclusion of both tests the data is indeed from an exponential distribution.

```{r}
lambda = 1 / mean(seeded)
T1 = mean(seeded)
z_a = qnorm(.975)
ci_r =mean(seeded)+z_a*(sd(seeded)/sqrt(26))
ci_l =mean(seeded)-z_a*(sd(seeded)/sqrt(26))
B=1000
Tstar=numeric(B)
for(i in 1:B) {
     Xstar=rexp(seeded,lambda)
     Tstar[i]=median(Xstar) }
pl=sum(Tstar<T1)/B; pr=sum(Tstar>T1)/B; p=2*min(pl,pr)
pl;pr;p

ks.test(seeded, pexp(26, lambda))

```

#### **2D**

Using an appropriate test, verify whether the median precipitation for seeded clouds is less than 300. Next, design and perform a test to check whether the fraction of the seeded clouds with the precipitation less than 30 is at most 25%.

```{r}
sum(seeded<300) 
binom.test(17,26,p=0.5) #H0 is therefore not rejected, which means that at the median  precipitation for seeded clouds is less than 300 is indeed at the given median.  


#there are different way to check this whereas the first method is:
quantile(seeded)

#the binomial method could also be used: 
binom.test(17,26,p=0.25, alternative = "l")
#just as the propportion test
prop.test(17,26,p=0.25, alt="l")
```

## Question 3

**Loading the dataset**

```{r}
dogs_data = read.table("dogs.txt",header=TRUE)
Isofluorane = dogs_data$isofluorane
Halothane = dogs_data$halothane
Cyclopropane = dogs_data$cyclopropane

```

#### **3A**

To check for normality of data distribution across the 3 variables in each record:

Visually, we plot the histogram and qq-plot for each variable to inspect for normality.

Mathematically, we can go through Shapiro-Wilk normality test to see if the

**Histograms**

```{r}
par(mfrow=c(1,3))
hist(Isofluorane,main="Histogram of Isofluorane data",probability = TRUE,xlab="Conc. in nanongrams per millimeter")
hist(Halothane,main="Histogram of Halothane data", probability = TRUE, xlab="Conc. in nanongrams per millimeter")
#par(mfrow=c(1,1))
hist(Cyclopropane,main="Histogram of Cycloprane data", probability = TRUE, xlab="Conc. in nanongrams per millimeter")
```

Looking visually at histograms, the data doesn't look to be distributed normally across any variable.

**QQ-Plots**

```{r}
par(mfrow=c(1,3))
qqnorm(Isofluorane,main="QQ-Plot of Isofluorane data")
qqline(Isofluorane)
qqnorm(Halothane,main="QQ-Plot of Halothane data")
qqline(Halothane)
qqnorm(Cyclopropane,main="QQ-Plot of Cyclopropane data")
qqline(Cyclopropane)
```

QQ-Plots reveal a better picture of the normality and looking at it, Halothane and Cyclopropane data look promising candidates for normality.

To confirm our inference, lets go through Shapiro-Wilk normality test.

**Shapiro-Wilk Normality Test**

```{r}
print(shapiro.test(Isofluorane))
print(shapiro.test(Halothane))
print(shapiro.test(Cyclopropane))
```

Halothane and Cyclopropane data have p-values \> 0.05, which means we can accept the Null Hypothesis $H_0$ that data is normally distributed. On the other hand, Isofluorane data isn't normally distributed as p-value \<= 0.05.

**Final Answer**

On visual inspection, all 3 variables in dataset didn't look visually normal using histogram inspection, but Halothane and Cyclopropane had somewhat straight qq-plots (variation due to low number of observations).

On closer inspection through Shapiro-Wilk Normality test, Isofluorane proved to be not normally distributed at all but Halothane and Cyclopropane concentrations were indeed normally distributed through the dataset.

#### **3B**

To check correlation between 2 normally distributed variables, we can apply Pearson's test for correlation. From 3-A, we realise that variable Isofluorane is not normally distributed. Therefore, we cannot use this test to check for its correlation with Halothane.

In this case, we will use Spearman's rank test for correlation.

```{r}

print(cor.test(Isofluorane,Halothane,method=c("spearman")))
```

Since obtained $\rho \neq 0$ , there might be some degree of positive correlation between both variables in our data. But p-value of 0.54 indicates that Null-Hypothesis is significantly supported, hence true $\rho = 0$ and it indicates no correlation between values of Isofluorane and Halothane. We need to note that p-value might not be correct due to presence of ties (same-values) in a fairly small sample size (10).

In 3-A, using Shapiro-Wilk Normality Test, it has already been concluded that variable Isofluorane doesn't come from a normal distribution while variable Halothane comes from a normal distribution.

```{r}
print(ks.test(Isofluorane,Halothane))
```

Using Kolmogorov-Smirnov Tests, we get a P-value of 0.76 indicating that 2 variable distributions might be similar. Again, due to low sample size and presence of ties, the calculated value of p-value can have some inaccuracies.

```{r}
print(paste("Length of datasamples : ",length(Isofluorane),length(Halothane),length(Cyclopropane)))
```

Since size of data sample is small (10\<30) and from 3-A we know 1st column does not follow normal distribution, we can say that Permutation tests are applicable in this case.

#### **3C**

Performing ANOVA test,

```{r}
#Creating DataFrame

df = data.frame(Concentration=as.vector(as.matrix(dogs_data)),An_Type=factor(rep(1:3,each=10)))

#Creating Linear Model
mod_1 = lm(Concentration ~ An_Type, data=df)

#Performing ANOVA Test
anova(mod_1)

#Printing output parameters of our Linear Model
summary(mod_1)
```

Looking at the p-value from the summary of the linear model and ANOVA test table, P-value = 0.011 \< 0.05, we can definitely say that One Way ANOVA test determines that the type of drug has a significant effect on the concentration of plasma epinephrine.

```{r}
confint(mod_1)
```

In the confidence level of 95%,

Estimated concentration of each type of drug is :

Isofluorane : 22.79 % to 64.01%

Halothane : 0.0% to 32.65 % (ignoring the negative Concentration value as it is not semantically possible)

Cyclopropane : 12.75% to 71.05%

#### **3D**

Performing the Kruskal-Wallis test :

```{r}
Concentration = df$Concentration
Anesthesia_Type = df$An_Type
kruskal.test(Concentration ~ Anesthesia_Type, data = df)
```

The p-value given using Kruskal-Wallis Test comes out to be 0.059 (\> 0.05). Hence the Null Hypothesis of this test is accepted, meaning it considers the effect of type of drug insignificant on the concentration of plasma epinephrine in the blood.

This is in contrast with the inference deduced using ANOVA test, although not by a far off margin since the p-value is very close 0.05 in case of Kruskal-Wallis test as well. This might be due to the nature of distribution as Kruskal-Wallis is a rank-based test, while ANOVA uses the nature of distribution such as Mean to compute the effect of the independent variable on the dependent variable. In our case, the first variable (Isofluorane) doesn't come from a standard normal distribution, while the other 2 variables do. Hence, Kruskal-Wallis might be a better representation of the effect of drugs on the plasma epinephrine concentration in our case since it is a non-parametric test.

## Question 4

Hemoglobin in trout

```{r, echo=FALSE, results='hide'}
data_h = read.table(file="hemoglobin.txt",header=TRUE)
data_h$rate = as.factor(data_h$rate)
data_h$method = as.factor(data_h$method)
data_h$hemoglobin = as.numeric(as.character(data_h$hemoglobin))
df = data.frame(data_h)
#df
```

#### **4A**

```{r, echo=FALSE, results='hide'}
I=4 #4 levels of rate
J=2 #2 levels of method
N=10 #80 observations/fishes
rbind(rep(1:I,each=N*J),rep(1:J,N*I),sample(1:(N*I*J)))
```

#### **4B**

Check for normality before using ANOVA-test. The data comes from normal distributions, since both the histogram and QQ-plot show a normally distributed graph. Besides, the shapiro-wilk test gives a p-value of 0.526 which is above 0.05 and therefore suggests that the data comes from normal distribution.

```{r, echo=FALSE, results='hide'}
model_h = lm(hemoglobin ~ rate*method, data=df)
#Visualizations on normality
par(mfrow=c(1,2))
hist(residuals(model_h), main='Histogram of residuals of model') 
qqnorm(residuals(model_h), main='QQ-Plot of residuals of model')
qqline(residuals(model_h))
#test on normality 
shapiro.test(residuals(model_h)) 
```

```{r, echo=FALSE, results='hide'}
#Performing Anova Test
anova(model_h)
summary(model_h)
```

From the two-way ANOVA test, we can see that the treatment variable 'rate' has a significant effect on hemoglobin, since the p-value is below 0.05. However, the block variable 'method' does not have a significant effect on hemoglobin since it's p-value is 0.216, which is above 0.05. The p-value for rate:method is 0.377, which means that there is no evidence for interaction.

#### **4C**

The factor 'rate' has the greatest influence on hemoglobin, since rate has a significant effect on hemoglobin and method does not have a significant effect on hemoglobin. However, since there is no interaction between rate and method, we have to remove the interaction term from the model and create an additive model where there is no interaction. Still, only the variable rate has a main effect on hemoglobin, and not the variable 'method'.

```{r, echo=FALSE, results='hide'}
model_new = lm(hemoglobin ~ rate+method, data=df)
anova(model_new)
summary(model_new)
#df
```

```{r, echo=FALSE, results='hide'}
methA = df[which(df$method == 'A'),]
methB = df[which(df$method == 'B'),]
rate_mA <- with(methA, tapply(hemoglobin, rate, mean))
rate_mB <- with(methB, tapply(hemoglobin, rate, mean))
rate_mA; rate_mB
```

The combination of rate 2 and method B leads to the highest mean of hemoglobin

Estimate the mean hemoglobin value for rate 3 by using method A.

```{r, echo=FALSE, results='hide'}
meth_a3 = df[which(df$method == 'A' & df$rate == 3),] #filter dataframe where rate=3 and method=A
hemo = meth_a3[,1] #select hemoglobin column
mean(hemo)
```

The mean of rate 3 and method A is 9.03

```{r, echo=FALSE, results='hide'}
rate_m <- with(df, tapply(hemoglobin, rate, mean))
rate_m
```

Rate 2 leads to the highest mean of hemoglobin.

#### **4D**

```{r, echo=FALSE, results='hide'}
model_1 = lm(hemoglobin ~ rate, data=df)
anova(model_1)
summary(model_1)
```

The results from the one-way ANOVA are exactly equal to the results of the two-way ANOVA. In both tests the effect of each rate on hemoglobin is tested and therefore, it is unnecessary to do the one-way ANOVA.

## Question 5

#### **5A**

To perform a three-way ANOVA test, we need to check normality. This is done by creating a QQ-plot and histogram of the residuals of the data. Additionally, the Shapiro-Wilk test was performed. The line in the QQ-plot looks linear and the distribution of the histogram can come from a normal distribution. Besides, the Shapiro-Wilk test had a p-value larger than 0.05 which means that the data comes from normal distribution. Therefore, a three-way ANOVA can be performed.

```{r, echo=FALSE, results='hide'}
data = read.table(file="cream.txt",header=TRUE)
data$batch = as.factor(data$batch)
data$position = as.factor(data$position)
data$starter = as.factor(data$starter)
data$acidity = as.numeric(as.character(data$acidity))
df = data.frame(data)
model  <- lm(acidity ~ batch+position+starter, data = df)
#Visualizations on normality
par(mfrow=c(1,2))
hist(residuals(model), main='Histogram of residuals of model') 
qqnorm(residuals(model), main='QQ-Plot of residuals of model')
qqline(residuals(model))
#test on normality 
shapiro.test(residuals(model)) 
#Performing Anova Test
anova(model)
summary(model)
```

Starter 1 is the intercept, which has a p-value of less than 0.05. Therefore there is a significant effect of starter 1 on acidity. While starter 2 has a p-value of 0.754, and therefore has no significant effect on acidity.

#### **5B**

Insignificant block variables, are variables that have a p-value larger than 0.05. So this is the block variable position, which has a p-value of 0.411

```{r, echo=FALSE, results='hide'}
model  <- lm(acidity ~ batch+starter, data = df)
#Performing Anova Test
anova(model)
summary(model)
```

After deleting the block variable position, starter 4 leads to a significantly different acidity.

#### **5C**

In the Friedman test the data does not need to come from a normal distribution, but it can also be used when the data comes from a normal distribution, like in this case. Instead of using the mean of the groups in an ANOVA test the Friedman makes use of ranks. However, this difference does not make any difference in the use of this test. Therefore, the Friedman test can also be used in this application to test whether there is an effect of starter on acidity. However, the Friedman test is not necessary when the ANOVA test has already been performed.

#### **5D**

Formula mixed effects is acidity \~ starter+(1\|batch). Where the 1\|batch is used to make from the block variable 'batch' a random effect variable.

```{r, echo=FALSE, results='hide'}
model_new  = lme4::lmer(acidity ~ starter+(1|batch) , data = df, REML=FALSE)
model  <- lm(acidity ~ batch+starter, data = df)
#Performing Anova Test
anova(model_new)
summary(model_new)
```

When using the fixed effects model the variables 1 and starter 4 had a significant effect on acidity. In the mixed effects model variables starter 1, starter 3, starter 4 had a significant effect on acidity. So, with the mixed effect model, more starters have a significant effect on acidity.
